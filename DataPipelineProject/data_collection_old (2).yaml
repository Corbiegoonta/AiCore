name: Data Collection Pipeline
description: |
  An implementation of an industry grade data collection pipeline that runs scalably in the cloud. 
  It uses Python code to automatically control your browser, extract information from a website, and store it on the cloud in a data warehouses and data lake.
  The system conforms to industry best practices such as being containerised in Docker and running automated tests.
milestones:
  - name: Set up the environment
    tasks:
      - name: Set up Github
        description: |
          In this project, you'll use GitHub to track changes to your code and save them online in a GitHub repo. Hit the button on the right to automatically create a new GitHub repo. We'll tell you when you need to use it as you go through the project.
        duration: 1
    description: Let's set up your dev environment to get started!
    id: 

  - name: Decide which website you are going to collect data from
    description: Choose a website that contains data you are interested in, to collect and build a dataset from
    tasks:
      - name: Select a website
        description: |
          Shortlist a few of your favourite products. 
          Do they display useful info that you could collect? 

          Can you think of any data that could be used to provide business value to them if it could be understood and modelled?


          Before you pick one, you should read a few pieces of advice:

          - Don't choose something because you think it sounds impressive. Instead, choose something you're passionate about.

          - Don't worry about picking something that seems like an uncommon data application. Some of our favourite projects have been from online book stores or retail brands.

          The data collection should be the simple part of this project. At the end of the day it doesn't matter how complicated your data is - what matters is building the system around it.

          Typical mistakes here include:

          - Spending more than 2 days thinking about this step. Decide quickly and get to work. It is likely that your code can be adapted to another data source later if you change your mind.

          - Letting things get too complex, too quickly. Many people initially decide that they want to do something like scrape the whole of Amazon. Even a few hundred datapoints can be enough. Get something simple working from end to end.

          - Trying to build something that is far too general. Focus. Zone in on a particular topic you are interested in. Don't try to scrape every category from every site. Laptops will have very different attributes to clothing items on Amazon.


          Some sites are easier to work with than others. So we've shortlisted a good range of options for you to work with which we know work well.
          Pick one to get started with.

          The following list was shortlisted by the team:
          - Holidays:
            - Agoda
            - Wego
          - Entertainment:
            - Rotten Tomatoes
            - IMDB
            - SoundCloud
            - Metacritic
            - A-Z Animals
          - Ecommerce:
            - Lego
            - Square Enix
            - Ikea
            - Trustpilot
            - Ocado
            - Waterstones
            - John Lewis
          - Health and Nutrition:
            - MyProtein
            - Lamberts
            - Gorilla Mind
          - Finance:
            - Coin Market
        duration: 4

  - name: Prototype finding the individual page for each entry
    description: Find links to the many pages that contain data which you want to collect.
    tasks:
      - name: Create a Scraper class
        description: |
          This class will contain all methods used to scrape data from your chosen website. 

          Once you have created your methods to navigate the website and get the required data, you should be able to initialise an instance of this class and use it to scrape the website selected. 

          Don't worry about adding all the methods just now, you will eventually populate the class with the necessary methods.
        duration: 6
      - name: Create different methods to navigate the webpage
        description: |
          This could include code to scroll the website, click a next button on product details page or simply navigate to the next required webpage. 

          Just think about what tasks you usually perform while browsing different websites.
        duration: 3
      - name: Implement a method to bypass cookies or login if required
        description: |
          Some websites will only allow you to collect the data after logging in or accepting cookie banners. 

          This can easily be automated. Once you've overcome that, move on. 

          Be careful, if there is no accept cookies button, the method will fail. 

          Think about how you can prevent the method from stopping the code running when it throws an exception.
        duration: 2
        
      - name: Create a method to get links to each page where the details can be found and store these in a list
        description: |
          Find the most efficient way to do this. 
          Don't have your code type into a textbox and then press enter if you could instead just figure out the pattern of the URLs generated for each search, and create them directly. 
          This list of urls will then be iterated through to scrape all data from each page.
        duration: 3

      - name: Run main body of code only within `if __name__ == "__main__"` block
        description: |
          Within the initialiser, have your class call all of the methods implemented so far.   


          Initialise the class within the `if __name__ == "__main__"` block, so that it only runs if this file is run directly rather than on any import, and make sure it all works.
        duration: 3

      - name: Begin documenting your experience
        description: |
          Now that you have chosen your website and built the initial scraper class, add documentation to your README file following this [guide](https://github.com/AI-Core/ExampleDocumentation).

          Make sure to include your reasoning for choosing your website, the technologies you've used etc.

        duration: 2

  - name: Retrieve data from details page
    description: Get all of the data for each record from each corresponding page
    tasks:
      - name: Create a function to retrieve text and image data from a single details page
        description: |
          What is each piece of information that you might want from this page? 

          Create one method to extract the image data and another to extract all text data from the page.

          For some websites there may not be useful image data to scrape.

          If this is the case, practice retrieving image data by collecting src links for a few images on the page, this could even just be scraping the websites logo. 

          Use XPath expressions to select the element and extract the text or src from each element.

          Print out the extracted data to ensure it's in the format you want and the methods are working as expected.
        duration: 3

      - name: Deterministically generate a unique ID
        description: |
          Usually, the unique string representing this product/page can be found in the URL, or an id for the product can be found somewhere on the page. 

          It's important that this id is deterministic (always the same for this product), as you will use it to prevent rescraping the same product data later on if you've already done so.

          It can be helpful to make this id user friendly for debugging purposes.

          A good example of this might be a product SKU or product number since they should be unique to the individual product. The idea is to create something unique so that specific records can be found and categorised easily.

          In the future you will be uploading your data to a structured database having a primary key column. 

          Entries in this column will need to be unique to identify each record uniquely, the friendly ID will help you implement this. 

          If there is no obvious way to generate a friendly ID you can use the website address as a fallback to uniquely identify the records. 

          Update your dictionary with the friendly ID.
        duration: 4

      - name: Generate a v4 UUID to act as the globally unique ID for this entry
        description: |
          Along with the user friendly id, it is typical to reference each record with a universally unique ID (UUID). 

          They can be easily generated with the python `uiid` package. 

          Use the package to generate a unique ID for each record and save this ID along with the products features. 

          You should use a version 4 UUID, which is the convention.

          You should look up how to generate a V4 UUID.
        duration: 2

      - name: Extract data and store in dictionary which maps feature name to feature value
        description: |
          Your dictionary should include all details for each record, its unique ID and links to any images associated with each record.
        duration: 2

      - name: Save the raw data dictionaries locally
        description: |
          At some point in the future you may realise that you wanted to do something with this data which you don't realise now. 

          Because of that, it's useful to save all of this data so that if you do need it, you've got it.

          Firstly, write code to create a folder called `raw_data` in the root of your project if it doesn't yet exist.

          Within there, create a folder with the id of the product as it's name.

          In that folder, save the dictionary in a file called `data.json`.

          Creating the product folder and data file should be done programmatically as your code runs.

          Don't spend time now scraping as much data as you can, as you'll need to do this again later. Simply make sure you can store it locally.
        duration: 3

      - name: Create a method to find image links and download images if applicable
        description: |
          Now you should be able to find the link to each image in your webpage and download them. 

          Don't worry about downloading every image from the website as it can take a lot of space on your hard drive. Just knowing how to retrieve the images will be enough. 

          You might need to look at a library the `urllib` or `requests` to download them.

          Just search stack overflow for this, and copy the function which you find.

          It's critical to save the images, not just save their link. 

          Otherwise, if the website changes the URL at which these images are stored, your entire dataset will be useless.

          Inside the folder you previously created for each datapoint, create a folder called `images`.

          Name each image by `<id of product>_<order of image>.<image file extension>` so `<id of product>_1.jpg` for example, this will ensure if the image gets misplaced you know which tabular data it is associated with.

          If your data naturally falls into categories, have Python create a subfolder which has the subcategory as the name when it finds a new category.
        duration: 2


      - name: Document the experience gained from this milestone
        description: |
          Continuing to follow the guidelines for documentation, add your experience and insight to your README file.

          Talk about the methods you have added and the reasoning behind your approach.
        duration: 1

  - name: Documentation and testing
    description: Set up testing for all of the code that you've written so far to catch any bugs and ensure that every part works as expected.
    tasks:
      - name: Refactor and optimise current code
        description: |

          Refactoring will be a continuous and constant process, but this is the time to really scrutinise your code. 

          You can use the following list to make improvements:

          - Are your methods and variables named appropriately i.e `create_list_of_website_links()` instead of `links()`. `for element in web_element_list` instead of `for i in list`.
          - Look for areas of code being repeated. If you see repeated code this is an indication that a method/function could perform this task in these cases.
          - If you have longer methods, is it because they have multiple concerns contained within these methods? Does your method create a dictionary and upload to the cloud? Then you should have two separate methods, one for uploading and one to create the dictionary.
          - Are your methods only used within your class? Then they should be made private or protected.
          - Have you added a `if __name__ == “__main__”:` statement to your code?
          - Are your imports and `from` statements in a consistent order? For example, you can order them alphabetically, with `from` statements before `import`s. Whatever you choose, be consistent.
          - Are there nested loops within your code? This is likely not optimal. You could likely break this up into two separate loops.
          - Don't overly use `self` within your code; it can make it harder to test and debug.
          - If you have used recursion, did you implement memoisation or tabulation to make it more performant?
          - Have you used `import *` statement? Change it to the exact method or class to import.
          - Make sure all docstrings are consistent for all methods.
          - Have you added typing to your methods?
        duration: 3

      - name: Add docstrings to all functions
        description: |
          Select a consistent format to create your docstrings. 

          Do you prefer the way Google, numpy, or Epytext format? 

          Docstring all your methods so that they are easy to understand by other users of your scraper.
        duration: 3

      - name: Create a unit test for each of your public methods
        description: |
          These tests could be as simple as checking your method returns the correct data type or as complicated as checking your are getting all the required data from each url. 

          Think about what tests you feel would make your scraper as robust as possible and implement them.
        duration: 4

      - name: Create a file which runs all of your tests when you run it
        description: |

          Create a file that performs integration testing of your scraper. 

          Be careful with all the imports, as navigating through different folders is a bit of a challenge.
        duration: 2

      - name: Test your unit tests are passing for all of your public methods.
        description: |
          If your test is failing, think about why it might be failing. 

          Is it because the method isn't functioning as it should? 

          Or does the url you're testing against no longer exist? 

          Make changes to each piece of code until they are all passing their tests.
        duration: 1

      - name: Update your documentation
        description: Update your README file. Add documentation on unit testing and how testing works for your scraper.
        duration: 1

  - name: Scalably store the data
    description: Store your data in the cloud in a format that makes sense
    tasks:
      - name: Upload your raw data folder to S3
        description: |
          Firstly, create an S3 bucket through the AWS console.

          Update your code so that as it runs, it uploads your raw JSON data and image data for each record to this S3 bucket using the AWS Python SDK, `boto3`.
        duration: 5

      - name: Upload any tabular data to RDS
        description: |
          Create a free tier micro RDS database, remember to make it publicly available. 

          Create database tables and define the schema required to store each record.

          You can use pandas to create a dataframe for each record and then upload to RDS using `psycopg2` and `sqlalchemy`.

          Alternatively you could create a table in your RDS using `sqlalchemy` and write SQL statements define a table and it's schema then upload the data.

          At this point you may want to provide users the option to save data to your machine locally, upload directly to RDS or both.

          Change your code to allow users this option.
        duration: 6

      - name: Upload any image data to S3
        description: |
          This can be done directly by creating a method which uploads the data directly to S3 using boto3.
        duration: 2

      - name: Update your documentation
        description: |
          Add more documentation to your README file based on what you have done for this milestone. 

          Talk about the cloud services you have used and how you interact with them in your code using `boto3`.

          Don't forget to check the [example documentation](https://github.com/AI-Core/ExampleDocumentation) for more information and to guide you.
        duration: 1

  - name: Preventing re-scraping and getting more data
    description: Improve things by skipping over examples that have already been visited, then collect more data.
    tasks:
      - name: Check that your scraper can run without issues
        description: |
          Don't worry about the amount of samples you can extract, as long as it can extract several records without breaking.

          If your scraper is failing to get a specified amount, you may need to implement some try/except statements to bypass any urls which cause it to fail.

          You may want to implement printing something to the output console to track the scraper progress.
        duration: 1

      - name: Finish testing any additional public methods implemented
        description: |
          Add tests for any more public methods you have implemented and reruns all tests to ensure they pass.
        duration: 2

      - name: Scalably prevent rescraping by checking your remote database table of tabular data to see if you've already scraped from this link, or have already got a product with this user-friendly ID
        description: |
          Use `sqlalchemy` and `psycopg2` to create a connection to the remote database. Then use a SQL query to check if a record exits with the user-friendly ID or url link.
        duration: 2

      - name: Prevent duplicate images being collected
        description: |
          If your scraper scrapes images in a separate method, use the friendly ID to prevent the scraping of images as well.
        duration: 1

      - name: Ensure that your scraper can scrape all the data you wanted without stopping
        description: |
          Don't worry about the exact amount of data, each page is different, so set that amount according to your website.

          You could test if for a 100 samples or scrape the entire website. Try to test it for at least 100 samples, if it can get 100 it can likely get much more. 

          Change your methods to deal with any contingencies and rerun it until it gets the required samples.
        duration: 2

  - name: Containerising the scraper and running it on a cloud server
    description: To take steps towards running the system on the cloud, we need to package it together in a self-contained unit - a container.
    tasks:
      - name: Final refactoring of code
        duration: 2
        description: |
          Take the time now to evaluate whether you code could have been laid out better. If so, make the corresponding changes.

          You can follow this list as a guide:

          - Are your methods and variables named appropriately i.e `create_list_of_website_links()` instead of `links()`. `for element in web_element_list` instead of `for i in list`.
          - Look for areas of code being repeated. If you see repeated code this is an indication that a method/function could perform this task in these cases.
          - If you have longer methods, is it because they have multiple concerns contained within these methods? Does your method create a dictionary and upload to the cloud? Then you should have two separate methods, one for uploading and one to create the dictionary.
          - Are your methods only used within your class? Then they should be made private or protected.
          - Have you added a `if __name__ == “__main__”:` statement to your code?
          - Are your imports and `from` statements in a consistent order? For example, you can order them alphabetically, with `from` statements before `import`s. Whatever you choose, be consistent.
          - Are there nested loops within your code? This is likely not optimal. You could likely break this up into two separate loops.
          - Don't overly use `self` within your code; it can make it harder to test and debug.
          - If you have used recursion, did you implement memoisation or tabulation to make it more performant?
          - Have you used `import *` statement? Change it to the exact method or class to import.
          - Make sure all docstrings are consistent for all methods.
          - Have you added typing to your methods?

      - name: Check all tests are passing
        duration: 1
        description: |
          Run all tests and ensure they are passing.

          If not, make the necessary changes to fix them.

      - name: Run the scraper in headless mode
        description: |
          Add headless as an additional flag to your webdrivers options and test the scraper works in headless mode. 

          Running your scraper in headless mode without the Graphical User Interface(GUI) will be required for the scraper to run within the a Docker container.

          You will need to add different options to your driver. 

          Take a look at this [link](https://stackoverflow.com/questions/50642308/webdriverexception-unknown-error-devtoolsactiveport-file-doesnt-exist-while-t) for more information.
        duration: 2

      - name: Create a Docker image which runs the scraper
        description: |
          Create a Dockerfile to build your scraper image locally.

          It will need to include instructions to:

          1. Choose a base image

          2. Put everything required by the scraper within the container

          3. Install any dependencies

          4. Run the main Python file

          As well as anything else required by your implementation.

          Once you've done that, build the image.

          Also, if your scraper relies on selenium, you might want to take a look at [this file](https://aicore-files.s3.amazonaws.com/Foundations/DevOps/docker_selenium.md) to create the Dockerfile for Google Chrome.

          If you're using Firefox you may want to look at [this file](https://aicore-files.s3.amazonaws.com/Cloud-DevOps/docker_firefox.md).

          Run a container from your newly created image to ensure it is working locally before deploying to the cloud.

        duration: 6

      - name: Push the container to Docker Hub
        description: |
          If you haven't already got one, create a [Dockerhub](https://hub.docker.com) account.

          Once you're happy your container is running correctly push the Docker image to Docker Hub.
        duration: 3

      - name: Create a new EC2 instance to deploy your scraper
        description: |
          Navigate the the AWS dashboard and spin up an new EC2 instance where you will deploy your scraper.
        duration: 6

      - name: Run the scraper on an EC2 instance
        description: |
          Ensure your docker container runs headlessly, locally. Install docker on the EC2 instance so that you can pull your image from Docker Hub.

          If you want to run the container in the background, so that you can close your terminal connection to the instance, you will need to run the container in detached mode using the `-d` flag.

          Test the scraper runs on the EC2.
        duration: 3

      - name: Update your documentation
        description: |
          Update your README file with what you have done for this milestone.

          Talk about docker and how it works, your code refactorisation and the techniques you used to avoid rescraping data.
        duration: 1
  - name: Monitoring and alerting
    description: Set up remote monitoring for your system so you can keep an eye on it whilst it is running headlessly in the cloud.
    tasks:
      - name: Set up a Prometheus container to monitor your scraper
        description: |
          Create a docker container running Prometheus and configure its `prometheus.yml` config file. 

          Similar to the last task in the previous milestone, if you are going to run additional commands on the EC2 instance, you will need to run the container in the background.

          Don't worry if you are not able to see the metrics, we will get to that point in next tasks.
        duration: 3

      - name: Monitor the docker container that runs your scraper
        description: |
          Configure the daemon file for Docker as well as the prometheus.yml file to monitor the metrics of the container. 

          Watch this video to see how to do this [here](https://youtu.be/0XLliK-G92w).
        duration: 1

      - name: Observe the metrics
        description: |
          If everything is working, Prometheus should be available to view metrics on port 9090 on your EC2 instance.

          You will need to configure your EC2 security group to allow access to this port. 

          You can then view Prometheus from `<IP address of the EC2 instance>:9090`.

          Refer to the video included in the previous task to see how to do this.
        duration: 1

      - name: Create a Grafana dashboard for these metrics
        description: |
          Create dashboard in Grafana to monitor the metrics of the containers and the hardware metrics of the EC2 instance.
        duration: 3

      - name: Update your documentation
        description: |
          Update your README file with everything you have done for this milestone, making sure to talk about prometheus and Grafana and how it links with your scraper.

          Continue to reference the example documentation, while making sure your docs are clear and concise.
        duration: 1

  - name: Set up a CI/CD pipeline for your Docker image
    description: Create a CI/CD pipeline to build and deploy your Docker image to DockerHub.
    tasks:
      - name: Set up the GitHub secrets
        description: |
          Set up the relevant GitHub secrets that contains the credentials required to push to your Dockerhub account.

          Check out which secret variables are required [here](https://docs.docker.com/ci-cd/github-actions/).
        duration: 5

      - name: Create the GitHub action
        description: |
          Create a GitHub action that is triggered on a `push` to the `main` branch of your repository. 

          The action needs to build the Docker image and push it to your Dockerhub account.
        duration: 1

      - name: Restart your scraper
        description: |
          On the EC2 instance, create some cronjobs to restart the scraper every day. 

          The cronjobs should stop and kill the container, and pull the latest image from your Dockerhub account.

        duration: 3

      - name: Update your documentation
        description: |
          Update your README file with what you have done for this milestone.

          Talk about CI/CD pipelines and the process you have developed.

          Go over your entire docs and make sure they read well and are clear and concise.
        duration: 1
